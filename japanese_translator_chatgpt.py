import openai
import json
import re
from docx import Document
from docx.shared import RGBColor
import time
import os
from typing import List, Dict, Optional, Tuple
import hashlib

# Regex patterns for Japanese text detection
JAPANESE_PATTERN = re.compile(r'[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9fff]+')

class JapaneseToVietnameseTranslatorChatGPT:
    def __init__(self, api_key: str, model: str = "gpt-3.5-turbo"):
        """Initialize translator with ChatGPT API key"""
        if not api_key or api_key == "your_openai_api_key_here":
            raise ValueError("Please provide a valid OpenAI API key")
        
        self.client = openai.OpenAI(api_key=api_key)
        self.model = model  # gpt-3.5-turbo hoáº·c gpt-4
        self.translation_cache = {}
        self.request_count = 0
        self.total_tokens_used = 0
        self.estimated_cost = 0.0
        
        # Pricing (USD per 1M tokens) - cáº­p nháº­t thÃ¡ng 7/2025
        self.pricing = {
            # Legacy models (converted to per 1K for compatibility)
            "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},  # $0.50/$1.50 per 1M
            "gpt-4": {"input": 0.03, "output": 0.06},               # $30/$60 per 1M
            "gpt-4-turbo": {"input": 0.01, "output": 0.03},         # $10/$30 per 1M
            
            # New models (per 1K tokens for compatibility)
            "gpt-4o": {"input": 0.005, "output": 0.015},            # $5/$15 per 1M
            "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},    # $0.15/$0.60 per 1M (RECOMMENDED)
            "gpt-4.1": {"input": 0.002, "output": 0.008},           # $2.00/$8.00 per 1M
            "gpt-4.1-mini": {"input": 0.0004, "output": 0.0016},    # $0.40/$1.60 per 1M
            "gpt-4.1-nano": {"input": 0.0001, "output": 0.0004},    # $0.10/$0.40 per 1M
            "o3": {"input": 0.002, "output": 0.008},                # $2.00/$8.00 per 1M
            "o4-mini": {"input": 0.0011, "output": 0.0044}          # $1.10/$4.40 per 1M
        }
        
        # Batch settings tá»‘i Æ°u cho speed + accuracy
        # Má»¥c tiÃªu: Output < 32K tokens Ä‘á»ƒ trÃ¡nh limit, Ä‘á»§ context Ä‘á»ƒ dá»‹ch chÃ­nh xÃ¡c
        
        if "gpt-4" in self.model and "4o" not in self.model and "4.1" not in self.model:
            # GPT-4 legacy (8K context limit)
            self.max_batch_size = 6       # Nhá» vÃ¬ context háº¡n cháº¿
            self.max_batch_chars = 3000   # 3K chars â‰ˆ 4.5K tokens input + prompt
            self.batch_delay = 1.5        # Delay Ä‘á»ƒ trÃ¡nh rate limit
            self.max_output_tokens = 2000 # Output limit
            
        elif "gpt-4.1" in self.model:
            # GPT-4.1 series (1M context, 32K output limit)
            # Tá»‘i Æ°u: Input Ä‘á»§ lá»›n Ä‘á»ƒ cÃ³ context, output < 32K
            self.max_batch_size = 12      # 12 Ä‘oáº¡n/batch - cÃ¢n báº±ng tá»‘t
            self.max_batch_chars = 8000   # 8K chars â‰ˆ 12K input tokens
            self.batch_delay = 0.5        # Nhanh vÃ¬ cÃ³ context lá»›n
            self.max_output_tokens = 16000 # Output limit: 16K tokens (an toÃ n)
            
        elif "gpt-4o" in self.model:
            # GPT-4o (128K context)
            self.max_batch_size = 10      # 10 Ä‘oáº¡n/batch
            self.max_batch_chars = 6000   # 6K chars Ä‘á»ƒ trÃ¡nh context limit
            self.batch_delay = 0.8
            self.max_output_tokens = 8000 # Output limit
            
        else:
            # GPT-3.5-turbo vÃ  model khÃ¡c
            self.max_batch_size = 8       # 8 Ä‘oáº¡n/batch
            self.max_batch_chars = 5000   # 5K chars
            self.batch_delay = 1.0
            self.max_output_tokens = 4000 # Output limit
        
        print(f"ğŸ¤– Khá»Ÿi táº¡o ChatGPT Translator - Model: {self.model}")
        print(f"ğŸ“¦ Batch settings: {self.max_batch_size} paragraphs, {self.max_batch_chars} chars")
        print(f"ğŸ¯ Max output tokens: {self.max_output_tokens}")
        if self.model in self.pricing:
            input_price = self.pricing[self.model]["input"]
            output_price = self.pricing[self.model]["output"]
            print(f"ğŸ’° GiÃ¡: ${input_price:.4f}/${output_price:.4f} per 1K tokens")
        
    def has_japanese(self, text: str) -> bool:
        """Kiá»ƒm tra xem text cÃ³ chá»©a kÃ½ tá»± tiáº¿ng Nháº­t khÃ´ng"""
        if not text:
            return False
        return bool(JAPANESE_PATTERN.search(text))
    
    def clean_japanese_text(self, text: str) -> str:
        """LÃ m sáº¡ch text tiáº¿ng Nháº­t"""
        # Loáº¡i bá» whitespace thá»«a
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Loáº¡i bá» cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t khÃ´ng cáº§n thiáº¿t
        text = re.sub(r'[^\u3040-\u309f\u30a0-\u30ff\u4e00-\u9fff\s.,!?()ï¼ˆï¼‰ã€Œã€ã€ã€ã€ã€‚ï¼ï¼Ÿ]', '', text)
        
        return text
    
    def estimate_tokens(self, text: str) -> int:
        """Æ¯á»›c tÃ­nh sá»‘ tokens cá»§a text (Japanese text thÆ°á»ng ~1.5-2 tokens per character)"""
        # Rough estimation: 1 character â‰ˆ 1.5 tokens cho tiáº¿ng Nháº­t
        return int(len(text) * 1.5)
    
    def calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """TÃ­nh toÃ¡n chi phÃ­ dá»±a trÃªn sá»‘ tokens"""
        if self.model not in self.pricing:
            return 0.0
        
        pricing = self.pricing[self.model]
        input_cost = (input_tokens / 1000) * pricing["input"]
        output_cost = (output_tokens / 1000) * pricing["output"]
        return input_cost + output_cost
    
    def create_translation_batch(self, paragraphs: List[Tuple[str, str]]) -> List[List[Tuple[str, str]]]:
        """Táº¡o cÃ¡c batch tá»‘i Æ°u tá»« danh sÃ¡ch paragraphs vá»›i kiá»ƒm soÃ¡t output tokens"""
        batches = []
        current_batch = []
        current_chars = 0
        
        for para_id, text in paragraphs:
            cleaned_text = self.clean_japanese_text(text)
            if not cleaned_text or cleaned_text in self.translation_cache:
                continue
            
            text_length = len(cleaned_text)
            
            # Æ¯á»›c tÃ­nh output tokens cho batch hiá»‡n táº¡i + text má»›i
            estimated_batch_input = current_chars + text_length
            estimated_output_tokens = self.estimate_tokens(str(estimated_batch_input)) // 2  # Output thÆ°á»ng ngáº¯n hÆ¡n input
            
            # Kiá»ƒm tra cÃ¡c giá»›i háº¡n
            size_ok = len(current_batch) < self.max_batch_size
            chars_ok = estimated_batch_input <= self.max_batch_chars
            output_ok = estimated_output_tokens <= (self.max_output_tokens * 0.8)  # Äá»ƒ 20% buffer
            
            if size_ok and chars_ok and output_ok:
                current_batch.append((para_id, cleaned_text))
                current_chars += text_length
            else:
                # LÆ°u batch hiá»‡n táº¡i vÃ  táº¡o batch má»›i
                if current_batch:
                    batches.append(current_batch)
                current_batch = [(para_id, cleaned_text)]
                current_chars = text_length
        
        # ThÃªm batch cuá»‘i cÃ¹ng
        if current_batch:
            batches.append(current_batch)
        
        return batches
    
    def translate_batch(self, batch: List[Tuple[str, str]]) -> Dict[str, str]:
        """Dá»‹ch má»™t batch cÃ¡c paragraphs cÃ¹ng lÃºc"""
        if not batch:
            return {}
        
        # Táº¡o unique ID cho má»—i Ä‘oáº¡n Ä‘á»ƒ tracking
        batch_items = []
        for i, (para_id, text) in enumerate(batch):
            unique_id = f"PARA_{i+1:03d}"
            batch_items.append((unique_id, para_id, text))
        
        # Táº¡o combined prompt
        japanese_texts = []
        for unique_id, para_id, text in batch_items:
            japanese_texts.append(f"[{unique_id}] {text}")
        
        combined_text = "\n\n".join(japanese_texts)
        
        prompt = f"""Báº¡n lÃ  má»™t chuyÃªn gia dá»‹ch thuáº­t tiáº¿ng Nháº­t sang tiáº¿ng Viá»‡t. HÃ£y dá»‹ch CHÃNH XÃC cÃ¡c Ä‘oáº¡n vÄƒn sau Ä‘Ã¢y.

QUAN TRá»ŒNG:
- Má»—i Ä‘oáº¡n cÃ³ ID riÃªng [PARA_XXX]
- Pháº£i tráº£ vá» ÄÃšNG Ä‘á»‹nh dáº¡ng: [PARA_XXX] Báº£n dá»‹ch tiáº¿ng Viá»‡t
- Dá»‹ch tá»± nhiÃªn, khÃ´ng mÃ¡y mÃ³c
- Giá»¯ nguyÃªn ngá»¯ cáº£nh vÃ  Ã½ nghÄ©a
- DÃ¹ng tá»« chuyÃªn ngÃ nh cÆ¡ khÃ­, ká»¹ thuáº­t náº¿u cÃ³
- KHÃ”NG thÃªm giáº£i thÃ­ch hay chÃº thÃ­ch gÃ¬ khÃ¡c

VÄƒn báº£n tiáº¿ng Nháº­t cáº§n dá»‹ch:

{combined_text}

Báº£n dá»‹ch tiáº¿ng Viá»‡t (giá»¯ Ä‘Ãºng format [PARA_XXX]):"""

        try:
            # Æ¯á»›c tÃ­nh tokens trÆ°á»›c khi gá»i API
            estimated_input_tokens = self.estimate_tokens(prompt)
            estimated_output_tokens = self.estimate_tokens(combined_text) // 2  # Output thÆ°á»ng ngáº¯n hÆ¡n
            estimated_cost = self.calculate_cost(estimated_input_tokens, estimated_output_tokens)
            
            print(f"ğŸ“¤ Batch {len(batch)} items - Input: ~{estimated_input_tokens} tokens")
            print(f"   Est Output: ~{estimated_output_tokens} tokens (limit: {self.max_output_tokens})")
            print(f"   Est Cost: ~${estimated_cost:.4f}")
            
            # Kiá»ƒm tra náº¿u estimated output gáº§n limit
            if estimated_output_tokens > self.max_output_tokens * 0.9:
                print(f"   âš ï¸ Output gáº§n limit, cÃ³ thá»ƒ cáº§n chia nhá» batch")
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system", 
                        "content": "Báº¡n lÃ  chuyÃªn gia dá»‹ch thuáº­t tiáº¿ng Nháº­t sang tiáº¿ng Viá»‡t chuyÃªn nghiá»‡p, dá»‹ch chÃ­nh xÃ¡c vÃ  tá»± nhiÃªn."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=self.max_output_tokens,  # Sá»­ dá»¥ng dynamic max_tokens
            )
            
            # Cáº­p nháº­t thá»‘ng kÃª
            self.request_count += 1
            if hasattr(response, 'usage'):
                input_tokens = response.usage.prompt_tokens
                output_tokens = response.usage.completion_tokens
                self.total_tokens_used += input_tokens + output_tokens
                actual_cost = self.calculate_cost(input_tokens, output_tokens)
                self.estimated_cost += actual_cost
                
                print(f"âœ… Response: {input_tokens} + {output_tokens} tokens = ${actual_cost:.4f}")
            
            # Parse káº¿t quáº£
            translations = {}
            if response.choices and response.choices[0].message.content:
                content = response.choices[0].message.content.strip()
                
                # TÃ¡ch theo lines vÃ  tÃ¬m pattern [PARA_XXX]
                lines = content.split('\n')
                for line in lines:
                    line = line.strip()
                    if line.startswith('[PARA_') and ']' in line:
                        try:
                            end_bracket = line.index(']')
                            unique_id = line[1:end_bracket]
                            translation = line[end_bracket + 1:].strip()
                            
                            # TÃ¬m text gá»‘c tÆ°Æ¡ng á»©ng vá»›i unique_id
                            for uid, para_id, original_text in batch_items:
                                if uid == unique_id:
                                    # LÆ°u vÃ o cache vÃ  káº¿t quáº£
                                    self.translation_cache[original_text] = translation
                                    translations[original_text] = translation
                                    break
                        except (ValueError, IndexError) as e:
                            print(f"âš ï¸ Parse error for line: {line[:50]}... - {e}")
                            continue
            
            print(f"âœ… Batch completed: {len(translations)}/{len(batch)} translations extracted")
            return translations
            
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ Batch translation error: {e}")
            
            # Náº¿u lá»—i context length, thá»­ chia nhá» batch
            if "context_length_exceeded" in error_msg or "maximum context length" in error_msg:
                print(f"ğŸ”„ Context limit exceeded, splitting batch into smaller chunks...")
                
                # Chia batch thÃ nh 2 pháº§n nhá» hÆ¡n
                half_size = len(batch) // 2
                if half_size > 0:
                    batch1 = batch[:half_size]
                    batch2 = batch[half_size:]
                    
                    print(f"   ğŸ“¦ Splitting into {len(batch1)} + {len(batch2)} items")
                    
                    # Dá»‹ch tá»«ng pháº§n
                    result1 = self.translate_batch(batch1) if batch1 else {}
                    time.sleep(self.batch_delay)  # Delay giá»¯a cÃ¡c sub-batch
                    result2 = self.translate_batch(batch2) if batch2 else {}
                    
                    # Káº¿t há»£p káº¿t quáº£
                    result1.update(result2)
                    return result1
                else:
                    # Náº¿u batch chá»‰ cÃ³ 1 item, dÃ¹ng single translation
                    return self._fallback_single_translations(batch)
            
            # Náº¿u lá»—i rate limit, retry vá»›i delay lá»›n hÆ¡n
            elif "429" in error_msg or "Too Many Requests" in error_msg:
                print(f"â³ Rate limit hit, waiting 10 seconds before retry...")
                time.sleep(10)
                return self.translate_batch(batch)  # Retry once
            
            # CÃ¡c lá»—i khÃ¡c, fallback sang single translation
            else:
                return self._fallback_single_translations(batch)
    
    def _fallback_single_translations(self, batch: List[Tuple[str, str]]) -> Dict[str, str]:
        """Fallback: dá»‹ch tá»«ng Ä‘oáº¡n riÃªng láº» khi batch fails"""
        print(f"ğŸ”„ Fallback to single translations for {len(batch)} items")
        
        results = {}
        for para_id, text in batch:
            try:
                translation = self.translate_text_single(text)
                if translation and not translation.startswith("[Lá»—i dá»‹ch"):
                    results[text] = translation
                time.sleep(0.5)  # Delay nhá» giá»¯a cÃ¡c single requests
            except Exception as e:
                print(f"âŒ Single translation failed for {para_id}: {e}")
                continue
        
        return results
    
    def translate_text_single(self, japanese_text: str) -> Optional[str]:
        """Dá»‹ch má»™t Ä‘oáº¡n text Ä‘Æ¡n láº» (fallback method)"""
        if not japanese_text or not self.has_japanese(japanese_text):
            return None
        
        cleaned_text = self.clean_japanese_text(japanese_text)
        if not cleaned_text:
            return None
        
        # Kiá»ƒm tra cache
        if cleaned_text in self.translation_cache:
            print(f"âœ“ Cache hit: {cleaned_text[:50]}...")
            return self.translation_cache[cleaned_text]
        
        try:
            prompt = f"""Dá»‹ch Ä‘oáº¡n vÄƒn tiáº¿ng Nháº­t sau sang tiáº¿ng Viá»‡t má»™t cÃ¡ch tá»± nhiÃªn vÃ  chÃ­nh xÃ¡c:

YÃªu cáº§u:
- Dá»‹ch chÃ­nh xÃ¡c nghÄ©a vÃ  ngá»¯ cáº£nh
- Sá»­ dá»¥ng tá»« ngá»¯ tiáº¿ng Viá»‡t tá»± nhiÃªn, khÃ´ng mÃ¡y mÃ³c  
- Giá»¯ nguyÃªn cáº¥u trÃºc cÃ¢u há»£p lÃ½
- KhÃ´ng thÃªm giáº£i thÃ­ch hay chÃº thÃ­ch
- Chá»‰ tráº£ vá» báº£n dá»‹ch tiáº¿ng Viá»‡t
- Dá»‹ch cÃ¡c tá»« chuyÃªn ngÃ nh cÆ¡ khÃ­, ká»¹ thuáº­t náº¿u cÃ³
- KhÃ´ng dá»‹ch cÃ¡c kÃ½ tá»± khÃ´ng pháº£i tiáº¿ng Nháº­t

VÄƒn báº£n tiáº¿ng Nháº­t:
{cleaned_text}

Báº£n dá»‹ch tiáº¿ng Viá»‡t:"""

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Báº¡n lÃ  chuyÃªn gia dá»‹ch thuáº­t tiáº¿ng Nháº­t sang tiáº¿ng Viá»‡t."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1000,
            )
            
            if response.choices and response.choices[0].message.content:
                translation = response.choices[0].message.content.strip()
                
                # Cáº­p nháº­t thá»‘ng kÃª
                self.request_count += 1
                if hasattr(response, 'usage'):
                    tokens = response.usage.prompt_tokens + response.usage.completion_tokens
                    self.total_tokens_used += tokens
                    cost = self.calculate_cost(response.usage.prompt_tokens, response.usage.completion_tokens)
                    self.estimated_cost += cost
                
                # LÆ°u cache
                self.translation_cache[cleaned_text] = translation
                
                print(f"âœ“ Single translated: {cleaned_text[:50]}... â†’ {translation[:50]}...")
                return translation
            
        except Exception as e:
            print(f"âœ— Single translation error: {e}")
            return f"[Lá»—i dá»‹ch: {str(e)}]"
        
        return None
    
    def add_translation_to_paragraph(self, paragraph, vietnamese_text: str):
        """ThÃªm báº£n dá»‹ch tiáº¿ng Viá»‡t vÃ o paragraph"""
        # ThÃªm xuá»‘ng dÃ²ng
        paragraph.add_run().add_break()
        
        # ThÃªm text tiáº¿ng Viá»‡t vá»›i format Ä‘áº·c biá»‡t
        vn_run = paragraph.add_run(f"ğŸ‡»ğŸ‡³ {vietnamese_text}")
        
        # Äá»‹nh dáº¡ng cho text tiáº¿ng Viá»‡t
        vn_run.font.italic = True
        vn_run.font.color.rgb = RGBColor(0, 100, 0)  # MÃ u xanh lÃ¡ Ä‘áº­m
        vn_run.font.size = vn_run.font.size  # Giá»¯ nguyÃªn size
    
    def collect_japanese_paragraphs(self, doc: Document) -> List[Tuple[str, str, object]]:
        """Thu tháº­p táº¥t cáº£ paragraphs tiáº¿ng Nháº­t tá»« document"""
        japanese_paragraphs = []
        
        # Thu tháº­p tá»« paragraphs chÃ­nh
        for i, paragraph in enumerate(doc.paragraphs):
            if self.has_japanese(paragraph.text.strip()):
                para_id = f"main_{i}"
                japanese_paragraphs.append((para_id, paragraph.text.strip(), paragraph))
        
        # Thu tháº­p tá»« tables
        for table_idx, table in enumerate(doc.tables):
            for row_idx, row in enumerate(table.rows):
                for cell_idx, cell in enumerate(row.cells):
                    for para_idx, paragraph in enumerate(cell.paragraphs):
                        if self.has_japanese(paragraph.text.strip()):
                            para_id = f"table_{table_idx}_{row_idx}_{cell_idx}_{para_idx}"
                            japanese_paragraphs.append((para_id, paragraph.text.strip(), paragraph))
        
        return japanese_paragraphs
    
    def process_word_document(self, input_path: str, output_path: str):
        """Xá»­ lÃ½ file Word vá»›i batch translation tá»‘i Æ°u"""
        print(f"ğŸ”„ Äang xá»­ lÃ½ tÃ i liá»‡u: {input_path}")
        start_time = time.time()
        
        try:
            doc = Document(input_path)
            
            # Thu tháº­p táº¥t cáº£ paragraphs tiáº¿ng Nháº­t
            print("ğŸ“‹ Äang thu tháº­p paragraphs tiáº¿ng Nháº­t...")
            japanese_paragraphs = self.collect_japanese_paragraphs(doc)
            
            total_japanese = len(japanese_paragraphs)
            print(f"ğŸ“ TÃ¬m tháº¥y {total_japanese} paragraphs tiáº¿ng Nháº­t")
            
            if total_japanese == 0:
                print("â„¹ï¸ KhÃ´ng cÃ³ text tiáº¿ng Nháº­t nÃ o Ä‘á»ƒ dá»‹ch")
                return
            
            # Táº¡o batches tá»‘i Æ°u
            print("ğŸ”§ Äang táº¡o batches tá»‘i Æ°u...")
            paragraph_data = [(para_id, text) for para_id, text, _ in japanese_paragraphs]
            batches = self.create_translation_batch(paragraph_data)
            
            print(f"ğŸ“¦ Táº¡o Ä‘Æ°á»£c {len(batches)} batches")
            
            # Æ¯á»›c tÃ­nh chi phÃ­ trÆ°á»›c khi báº¯t Ä‘áº§u
            total_chars = sum(len(text) for _, text, _ in japanese_paragraphs)
            estimated_tokens = self.estimate_tokens(str(total_chars))
            estimated_total_cost = self.calculate_cost(estimated_tokens, estimated_tokens // 2)
            print(f"ğŸ’° Æ¯á»›c tÃ­nh chi phÃ­: ~${estimated_total_cost:.4f}")
            
            # Xá»­ lÃ½ tá»«ng batch
            all_translations = {}
            for batch_idx, batch in enumerate(batches):
                print(f"\nğŸ”„ Processing batch {batch_idx + 1}/{len(batches)} ({len(batch)} items)")
                
                # Dá»‹ch batch
                batch_translations = self.translate_batch(batch)
                all_translations.update(batch_translations)
                
                # Delay giá»¯a cÃ¡c batches Ä‘á»ƒ trÃ¡nh rate limit
                if batch_idx < len(batches) - 1:
                    print(f"â³ Waiting {self.batch_delay}s before next batch...")
                    time.sleep(self.batch_delay)
            
            # Ãp dá»¥ng translations vÃ o document
            print(f"\nğŸ“ Äang Ã¡p dá»¥ng {len(all_translations)} báº£n dá»‹ch vÃ o document...")
            translated_count = 0
            
            for para_id, original_text, paragraph_obj in japanese_paragraphs:
                cleaned_text = self.clean_japanese_text(original_text)
                if cleaned_text in all_translations:
                    vietnamese_text = all_translations[cleaned_text]
                    self.add_translation_to_paragraph(paragraph_obj, vietnamese_text)
                    translated_count += 1
                elif cleaned_text in self.translation_cache:
                    vietnamese_text = self.translation_cache[cleaned_text]
                    self.add_translation_to_paragraph(paragraph_obj, vietnamese_text)
                    translated_count += 1
            
            # LÆ°u document
            print("ğŸ’¾ Äang lÆ°u tÃ i liá»‡u...")
            save_start = time.time()
            doc.save(output_path)
            save_time = time.time() - save_start
            
            # LÆ°u cache
            cache_file = output_path.replace('.docx', '_translation_cache_chatgpt.json')
            self.save_cache(cache_file)
            
            total_time = time.time() - start_time
            
            # In bÃ¡o cÃ¡o káº¿t quáº£
            self.print_results(output_path, {
                'total_paragraphs': len(doc.paragraphs),
                'japanese_paragraphs': total_japanese,
                'translated_paragraphs': translated_count,
                'batches_processed': len(batches),
                'cache_size': len(self.translation_cache)
            }, total_time, save_time, cache_file)
            
        except Exception as e:
            print(f"âŒ Lá»—i xá»­ lÃ½ tÃ i liá»‡u: {e}")
            import traceback
            traceback.print_exc()
    
    def save_cache(self, cache_file: str):
        """LÆ°u cache dá»‹ch thuáº­t"""
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.translation_cache, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ÄÃ£ lÆ°u {len(self.translation_cache)} báº£n dá»‹ch vÃ o cache")
        except Exception as e:
            print(f"âŒ Lá»—i lÆ°u cache: {e}")
    
    def load_cache(self, cache_file: str):
        """Táº£i cache dá»‹ch thuáº­t cÃ³ sáºµn"""
        try:
            if os.path.exists(cache_file):
                with open(cache_file, 'r', encoding='utf-8') as f:
                    self.translation_cache = json.load(f)
                print(f"ğŸ“¥ ÄÃ£ táº£i {len(self.translation_cache)} báº£n dá»‹ch tá»« cache")
            else:
                print("ğŸ“ KhÃ´ng tÃ¬m tháº¥y cache, báº¯t Ä‘áº§u tá»« Ä‘áº§u")
        except Exception as e:
            print(f"âŒ Lá»—i táº£i cache: {e}")
    
    def print_results(self, output_path: str, stats: Dict, total_time: float, save_time: float, cache_file: str):
        """In bÃ¡o cÃ¡o káº¿t quáº£ vá»›i thÃ´ng tin chi phÃ­"""
        print("\n" + "="*60)
        print("ğŸ‰ Káº¾T QUáº¢ Dá»ŠCH THUáº¬T - CHATGPT VERSION")
        print("="*60)
        print(f"ğŸ“ File Ä‘áº§u ra: {output_path}")
        print(f"ğŸ¤– Model: {self.model}")
        print(f"ğŸ“Š Tá»•ng paragraphs: {stats['total_paragraphs']}")
        print(f"ğŸ‡¯ğŸ‡µ Paragraphs tiáº¿ng Nháº­t: {stats['japanese_paragraphs']}")
        print(f"ğŸ‡»ğŸ‡³ Paragraphs Ä‘Ã£ dá»‹ch: {stats['translated_paragraphs']}")
        print(f"ğŸ“¦ Batches Ä‘Ã£ xá»­ lÃ½: {stats['batches_processed']}")
        print(f"ğŸ’¾ Báº£n dá»‹ch trong cache: {stats['cache_size']}")
        print(f"ğŸ”„ API calls: {self.request_count}")
        print(f"ğŸ¯ Tokens sá»­ dá»¥ng: {self.total_tokens_used:,}")
        print(f"ğŸ’° Chi phÃ­ Æ°á»›c tÃ­nh: ${self.estimated_cost:.4f}")
        print(f"â±ï¸  Thá»i gian xá»­ lÃ½: {total_time:.2f} giÃ¢y")
        print(f"ğŸ’¾ Thá»i gian lÆ°u: {save_time:.2f} giÃ¢y")
        print(f"ğŸ“‚ Cache file: {cache_file}")
        
        if stats['japanese_paragraphs'] > 0:
            success_rate = (stats['translated_paragraphs'] / stats['japanese_paragraphs']) * 100
            print(f"âœ… Tá»· lá»‡ thÃ nh cÃ´ng: {success_rate:.1f}%")
            
            # TÃ­nh hiá»‡u quáº£ batch
            if stats['batches_processed'] > 0:
                avg_per_batch = stats['translated_paragraphs'] / stats['batches_processed']
                print(f"ğŸ“Š Trung bÃ¬nh: {avg_per_batch:.1f} paragraphs/batch")
        
        print("="*60)

def main():
    """HÃ m chÃ­nh"""
    # Import config tá»« file riÃªng
    try:
        from config_chatgpt import get_config, print_config
        CONFIG = get_config()
        if CONFIG is None:
            return
        print_config()
    except ImportError:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y file config_chatgpt.py!")
        print("   Vui lÃ²ng táº¡o file config_chatgpt.py vá»›i OpenAI API key")
        return
    
    print("ğŸŒŸ JAPANESE TO VIETNAMESE TRANSLATOR - CHATGPT VERSION")
    print("="*60)
    print(f"ğŸ“¥ Input: {CONFIG['input_file']}")
    print(f"ğŸ“¤ Output: {CONFIG['output_file']}")
    print(f"ğŸ¤– Model: {CONFIG.get('model', 'gpt-3.5-turbo')}")
    print()
    
    # Kiá»ƒm tra file input
    if not os.path.exists(CONFIG['input_file']):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file input: {CONFIG['input_file']}")
        return
    
    try:
        # Khá»Ÿi táº¡o translator
        print("ğŸ”§ Äang khá»Ÿi táº¡o ChatGPT translator...")
        translator = JapaneseToVietnameseTranslatorChatGPT(
            api_key=CONFIG['api_key'],
            model=CONFIG.get('model', 'gpt-3.5-turbo')
        )
        
        # Táº£i cache cÃ³ sáºµn
        translator.load_cache(CONFIG['cache_file'])
        
        # Xá»­ lÃ½ tÃ i liá»‡u
        translator.process_word_document(CONFIG['input_file'], CONFIG['output_file'])
        
        print("\nğŸŠ Dá»ŠCH THUáº¬T HOÃ€N THÃ€NH!")
        print(f"ğŸ’° Tá»•ng chi phÃ­: ${translator.estimated_cost:.4f}")
        
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
